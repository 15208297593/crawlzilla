#!/bin/bash
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`

# export path
. ../conf/crawlzilla-env.sh

function nutch_crawl_job(){
    # nutch crawl
    s_time=`date '+%s'`
    mkdir -p $CRAWLDB/$crawl_DB_name/meta/
    echo $s_time > $CRAWLDB/$crawl_DB_name/meta/start_time
    show_echo "$NUTCH_HOME/bin/nutch crawl $URLS -dir $CRAWLDB/$crawl_DB_name -depth $depth -topN 5000"
    show_echo "nutch solr index ..."
    nutch_solr_index
    f_time = `date '+%s'`
    echo $f_time > $CRAWLDB/$crawl_DB_name/meta/finish_time

}

function nutch_solr_index(){
    ## add solr collection
    # cp collection
    # cp -r $SOLR_HOME/example/solr/collection1 $SOLR_HOME/example/solr/$crawl_DB_name
    # add describe to solr.xml
    # sed -i ...
    Solr_Str = "<core schema=\"schema.xml\" instanceDir="$crawl_DB_name/" name="$crawl_DB_name" config=\"solrconfig.xml\" dataDir=\"data\"/>"
    # run..
    show_echo "/bin/nutch solrindex http://127.0.0.1:8983/solr/***TODO*** $CRAWLDB/$crawl_DB_name/crawldb -linkdb $CRAWLDB/$crawl_DB_name/linkdb $CRAWLDB/$crawl_DB_name/segments/*"
}

# create webmanager of crawlzilla


function main(){
  case $1 in
    "crawljob")
        show_echo "crawljob"
        crawl_DB_name=$2
        depth=$3
        if [ "$crawl_DB_name" != "" ] && [ "$depth" != "" ] ; then
	   nutch_crawl_job $crawl_DB_name $depth
        else
            show_echo "Please key in crawl DB name and depth!"
        fi
    ;;
  *)
    show_echo "Welcome to Use CrawlJob Shell!"
    show_echo "Usage: crawljob DBName depth"
    exit 1
  esac
}

main $1 $2 $3
