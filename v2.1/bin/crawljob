#!/bin/bash
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`
cd $bin
# export path
. ../conf/crawlzilla-env.sh

function nutch_crawl_job(){
    # nutch crawl
    
    log_echo "Start CrawlJob: $crawl_DB_name" $LOG_HOME/$crawl_DB_name.log
    s_time=`date '+%s'`
    create_time=`date '+%Y-%m-%d %R'`
    mkdir -p $CRAWLDB/$crawl_DB_name/.meta/
    echo $s_time > $CRAWLDB/$crawl_DB_name/.meta/start_time
    echo $create_time > $CRAWLDB/$crawl_DB_name/.meta/create_time
    echo $depth > $CRAWLDB/$crawl_DB_name/.meta/depth
    cp -r $URLS $CRAWLDB/$crawl_DB_name/.meta/
    show_echo "$NUTCH_HOME/bin/nutch crawl $URLS -dir $CRAWLDB/$crawl_DB_name -depth $depth -topN 5000"
    echo "$NUTCH_HOME/bin/nutch crawl $URLS -dir $CRAWLDB/$crawl_DB_name -depth $depth -topN 5000" >> /tmp/log
    $NUTCH_HOME/bin/nutch crawl $URLS -dir $CRAWLDB/$crawl_DB_name -depth $depth -topN 5000
    show_echo "nutch solr index ..."
    nutch_solr_index
    f_time=`date '+%s'`
    echo $f_time > $CRAWLDB/$crawl_DB_name/meta/finish_time
}

function nutch_solr_index(){
    ## add solr collection
    # cp collection
    cp -r $SOLR_HOME/example/solr/collection1 $SOLR_HOME/example/solr/$crawl_DB_name
    # add describe to solr.xml
    # sed -i ...
    insertLineNO=`cat -n /opt/crawlzilla/solr/example/solr/solr.xml | grep crawlzilla_end | awk 'NR==1 {print "" $1}'`
    insertLineNO=$(($insertLineNO-1))
    show_echo $insertLineNO

    Solr_Str="<core schema=\"schema.xml\" instanceDir="$crawl_DB_name/" name="$crawl_DB_name" config=\"solrconfig.xml\" dataDir=\"data\"/>"

    sed -i ''$insertLineNO'a  <core schema=\"schema.xml\" instanceDir="'$crawl_DB_name'/" name="'$crawl_DB_name'" config=\"solrconfig.xml\" dataDir=\"data\"/> ' $SOLR_HOME/example/solr/solr.xml
    show_echo "sed -i ''$insertLineNO'a '$Solr_Str'' $SOLR_HOME/example/solr/solr.xml"

    # run..
    show_echo "/bin/nutch solrindex http://127.0.0.1:8983/solr/$crawl_DB_name $CRAWLDB$crawl_DB_name/crawldb -linkdb $CRAWLDB$crawl_DB_name/linkdb $CRAWLDB$crawl_DB_name/segments/*"
        
   # restart solr 
    $bin/services solr restart

   echo "$NUTCH_HOME/bin/nutch solrindex http://127.0.0.1:8983/solr/$crawl_DB_name $CRAWLDB$crawl_DB_name/crawldb -linkdb $CRAWLDB$crawl_DB_name/linkdb $CRAWLDB$crawl_DB_name/segments/*"  >> /tmp/log
   $NUTCH_HOME/bin/nutch solrindex http://127.0.0.1:8983/solr/$crawl_DB_name $CRAWLDB/$crawl_DB_name/crawldb -linkdb $CRAWLDB/$crawl_DB_name/linkdb $CRAWLDB/$crawl_DB_name/segments/*
}

# create webmanager of crawlzilla

# delete search engine
function delete_search_DB(){
  show_echo "delete crawlDB!"
  check_DB_exist
  if [ $db_exist_flag = 1  ]; then
    show_echo "db exist, delete DB $DB_name"
    echo "rm -rf $CRAWLDB/$DB_name"
    echo "rm -rf /opt/crawlzilla/solr/example/solr/$DB_name"
    delete_line_NO=`cat -n /opt/crawlzilla/solr/example/solr/solr.xml | grep $DB_name | awk 'NR==1 {print "" $1}'`
    echo $delete_line_NO
    echo "sed -i "$delete_line_NO"d $SOLR_HOME/example/solr/solr.xml"
  
  else
    show_echo "db not exist!"
  fi



  show_echo $db_exist_flag
  # crawlDB
  # check exist
  # rm -rf $CRAWLDB/$DB_Name
}

function check_DB_exist(){
    db_exist_flag=0
    show_echo $CRAWLDB/$DB_name
    if [  -d $CRAWLDB/$DB_name ]; then
      db_exist_flag=1
    fi
}


function main(){
  case $1 in
    "crawljob")
        show_echo "crawljob"
        crawl_DB_name=$2
        depth=$3
        if [ "$crawl_DB_name" != "" ] && [ "$depth" != "" ] ; then
	   nutch_crawl_job $crawl_DB_name $depth
        else
            show_echo "Please key in crawl DB name and depth!"
        fi
    ;;
    "delete")
        DB_name=$2
        show_echo "delete search engine: $DB_name"
        delete_search_DB
    ;;
  *)
    show_echo "Welcome to Use CrawlJob Shell!"
    show_echo "Usage: crawljob DBName depth"
    exit 1
  esac
}

main $1 $2 $3
#nutch_solr_index
